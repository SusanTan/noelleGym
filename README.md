# NOELLE artifact evaluation

This repository includes the evaluation materials for the NOELLE CGO 2022 paper: "NOELLE Offers Empowering LLVM Extensions".

## Artifact

This artifact includes the evaluation of the data that supports the version of the paper that was submitted in September.
Next you can find the instructions to reproduce the related results.

Since NOELLE is an ongoing project, we did not stop working on it after submission.
We improved NOELLE after submission to the point that we can now target more benchmarks and we can perform more evaluations.
This artifact also includes the capability to generate these extra evaluations and benchmarks.
Finally, these new evaluations and benchmarks will be included in the final version of the paper.

### Prerequisites 

The artifact is available as a docker image.

The docker image includes the following software:
- go (we use 1.13.7)
- LLVM (with clang) 9.0.0
- cmake 3
- awk
- sed
- bash


## Data collection
In this section, we will collect the data required to reproduce the figures in the paper.

### Run all evaluations (Estimated time: ? hours)

To run the artifact to generate all data needed to support the submitted version of the paper, run within the docker image
```
cd ~ ;
./run_me_submission.sh ;
```

To (optionally) run the artifact to generate all data needed to support the final version of the paper, run within the docker image
```
cd ~ ;
./run_me_final.sh ;
```

### Step-by-Step Guide for running a single evaluation
TODO

#### Generating Figure 3 (Estimated time: ? hours)
TODO

#### Generating Figure 4 (Estimated time: ? hours)
TODO

#### Generating Figure 5 (Estimated time: ? hours)
TODO

## Data organization
All the generated data can be found under `results`.
Data we generated in our machine can be found under `results/authors_machine`.
Data that is generated by running the artifact can be found under `results/current_machine`.

Both `results/authors_machine` and `results/current_machine` are organized in the same way.
They have a subdirectory per benchmark suite (e.g., `results/current_machine/PARSEC3` includes all data generated for the PARSEC-3.0 benchmarks).
Each benchmark suite has three sub-directories: 
- `dependences`: this sub-directory includes information about the dependences of the benchmark. This data is used to generate Figure 3. 
- `loops`: this sub-directory includes information about loops like their induction variables or they loop invariants. This data is used to generate Figure 4 and it will be used to generate a new figure in the final version of the paper. This new figure will compare the number of induction variables (per benchmark) detected by LLVM and those detected by NOELLE.
- `speedups`: this sub-directory includes execution times collected by running the benchmarks when compiled using vanilla `clang` and when compiled when using `clang` adding NOELLE optimizations in its middle-end. This data is used to generate Figure 5.
- `compilation`: this sub-directory includes new information that wasn't part of the original submission of the paper. This data is about the compilation time and memory consumption of the most important tools built upon NOELLE.

### Dependences
For each benchmark in a benchmark suite you will find a text file in the benchmark suite directory (e.g., `results/current_machine/PARSEC3/blackscholes.txt` )

### Loops
TODO

### Speedups
TODO

### (Optional) Compilation
TODO
